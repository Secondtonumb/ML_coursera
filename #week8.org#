** K-means algorithm
+ input number of clusters
+ training set {x^{(1)}, ..}

Randonly initialize K cluster centroids 
Repeat {
  for i = 1 to m
    c^{i}:= index(from 1 to k) of cluster centroid closest to x^{i}
  for k = 1 to K
    \mul_{k}:= average(mean) of points assigned to cluster k
}

+ CLuster assignment step
+ move centroid

*** K-means for non-separated clusters
+ 市场细分
** Optimazation objective
+ 追踪两个变量
+ 失真代价函数(distortion cost function)
随着iter增加，J一定是减小的

** Random Initialization

+ Should have K < m
+ Randomly pick K training examples
+ Set 1, ... k equal to these K examples
*** Local optima 局部最优解
好的方法 -> 多次随机initialize

循环多次
pick clustering that gave lowest cost J(c^{(1)}, ..., c^{(m)}, \mul_{1}, ..., \mul_{K})
** Choosing Number of Cluster
最好的方法仍然是手动选择
+ Elbow method 肘点
+ 有可能不存在肘点
+ 一般kmeans算法是为了下游的用处而进行处理的一种算法
+ 合适的方法是针对下游的问题来随机应对

** Motivation I:Data Compression
*** dimensiionality reduction

** Principal Component Analysis Problem Formulation
*** Data preprocessing
+ Training set:...
+ Preprocessing (feature scaling/ mean normalization):
均值归一化

replace x_{j} == x_{j} - \mul_{j}
If different features on different scales (e.g., x_1=size of house x_2=number of bedrooms), scale features to have comparable range of values
+ covariance matrix 先算出协方差矩阵
+ 求协方差矩阵的特征向量
SVD -> singular value decompostion
+ 协方差矩阵满足对陈正定 symmetric positive definite
*** Choosing the Number of Principal Components
+ Average squared projection error
+ Total variation in the data
Average squared projection / Total variation < \epsilon
+ 用SVD
1 - \sum_{i=1}^{k}S_{ii}/ \sum_{i=1}^{n}S_
** Advice for applying PCA
+ Supervised learning speedup
+ Bad use of PCA : To prevent overfitting
fewer features, less likely to overfit
+ How about doing the whole thing without using PCA
+ Before implementing PCA, first try running whatever you want to do with the original/raw data, Only if that doesn' t do what you want, then implement PCA and consider using z_{(i)}
