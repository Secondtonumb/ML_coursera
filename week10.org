* Gradient Descent with large Datasets
** Learning with Large Datasets
+ 假设你面临一个监督学习问题而且有一个很大的数据集，你将如何判断用全部的数据将会比只用其中的一小部分效果好?
+ 画一个关于样本个数m的学习曲线，并核对当m的值较小是算法的方差比较高。
** stochastic Gradient Descent 随机梯度下降
梯度下降法的问题是 当m值很大时，计算微分项的计算量就变得很大，因为需要对所有m个
训练样本求和
-> batch gradient descent 批量梯度下降
随机梯度下降 
+ shuffle data set
+ repeat GD
+ 等于这个cost函数关于参数\theta_{j} 的偏微分
+ 使修改后的参数对第一个训练样本的拟合好一些
完成一次内层循环之后再转向第二个训练样本使得参数对第二个的拟合也会好一些。
-> 通过打乱实际上可以让你的随机梯度下降稍微快一些收敛
+ SGD不需要等到对所有m个训练样本求和来得到梯度项，而是只需要对单个训练样本求出梯度项
+ 在随机梯度下降中，每一次迭代都会更快
+ 看起来是以某个比较随机，迂回的路径朝着全局最小值逼近
+ 随机梯度下降是在某个靠近全局最小值的区域内徘徊
+ 外层循环 只需要1-10次?
