* Gradient Descent with large Datasets
** Learning with Large Datasets
+ 假设你面临一个监督学习问题而且有一个很大的数据集，你将如何判断用全部的数据将会比只用其中的一小部分效果好?
+ 画一个关于样本个数m的学习曲线，并核对当m的值较小是算法的方差比较高。
** stochastic Gradient Descent 随机梯度下降
梯度下降法的问题是 当m值很大时，计算微分项的计算量就变得很大，因为需要对所有m个
训练样本求和
-> batch gradient descent 批量梯度下降
随机梯度下降 
+ shuffle data set
+ repeat GD
+ 等于这个cost函数关于参数\theta_{j} 的偏微分
+ 使修改后的参数对第一个训练样本的拟合好一些
完成一次内层循环之后再转向第二个训练样本使得参数对第二个的拟合也会好一些。
-> 通过打乱实际上可以让你的随机梯度下降稍微快一些收敛
+ SGD不需要等到对所有m个训练样本求和来得到梯度项，而是只需要对单个训练样本求出梯度项
+ 在随机梯度下降中，每一次迭代都会更快
+ 看起来是以某个比较随机，迂回的路径朝着全局最小值逼近
+ 随机梯度下降是在某个靠近全局最小值的区域内徘徊
+ 外层循环 只需要1-10次?
** Mini-batch gradient descent
+ Batch gradient descent: Use all m examples in each iteration
+ Stochastic gradient descent: Use 1 examples in each iteration
+ Mini-batch gradient descent: Use n examples in each iteration
*** Vectorization
注意考虑batchsize的大小，有时候可能回比随机梯度下降还要快。
** stochastic Gradient Descent COnvergence 收敛性
cost(\theta, (x^{(i)}, y^{(i)})) =  1 / 2 (h_{\theta}(x^{(i)} - y^{(i)}))^{2}
+ During learning, compute cost befor updating \thata using (x^{()}, y^{(i)})
+ 即可以用来检验算法是否正确，也可以用来调整学习率
* Advanced Topics
** Online Learning 
+ 如果我们找到了这样的一个Feature x capture properties of user, of origin/destination and asking price. We want to learn to optimize price
+ Get(x, y) corresponding to user.
+ Updata \theta using (x, y)
+ 放弃了固有数据集的概念
+ 也可以根据用户偏好进行调试
+ predict CTR 预测点击率 click-through rate
+ Some of the advantages of using an online learning algorithm are:
+ It can adapt to changing user tastes
+ It allows us to learn from a continuous stream of data, since we use each example once then no longer need to process it again
** Map Reduce and Data Parallelism
+ Map reduce 映射约减
+ 有的人认为Map reduce 方法和梯度下降法同等重要甚至更加重要。能够处理更大规模的问题
+ 将训练集分为几个子集
+ 类似并行处理
+ Map-reduce and summation over the training set
  + Many learning algorithm can be expressed as computing sums of functions over the training set

+ Multi-core machines
/open-source implementation/
