* Multivariate Linear Regression
** Multiple features
适用于更多变量，或更多特征量。
+ n = numbers of features
+ x^{(i)} = input(features) of training example
+ x_{j}*{(i)} = value of feature j in i^{th} training example

Hypothesis 假说
h(x) = \theta^{T}x
** Gradient Descent for multiple Variables
+ Gradient Descent 
   Previously(n = 1)
** Gradient Descent in Practice I - Feature Scaling
特征缩放
+ 如果两个特征量的尺度差距很大，代价函数的在小尺度次元上的受到的影响会很大，梯度下降的时候会在大尺度次元上来回震荡，会导致收敛很慢。
(contour maybe a very long oval)
+ *特征缩放* 将两个次元缩放到同一个尺度
i.e. : 0 <= x_1 <= 1, 0 <=x_2 <= 1
+ 一般来说喜欢取[-1, 1]
+ 尺度过大则缩小，尺度过小则放大，目的是保证尺度尽量一致。尽量在同一数量级。
*Mean normalization* 正则化
+ Replace x_1 with x_i - \mul_1 to make features have approximately zero mean
** Gradient Descent in Practice II - Learning Rate
学习率
+ Makingsure gradient descent is working correctly 
Value of cost function after each iteration of gradient decent, J(theta)should decrease after every iteration
/flatten out/ 变平
+ convergence test:
Declare convergence if J(thata) decrease by less than 10^{-3} in one iteration
+ 当J 不断增大的时候 一般认为我们需要一个更小的学习率
/overshooting the minimum/
/minuscult baby step/
** Features and Polynomial Regression
+ 可以自行创造某种特征
譬如房子的长度和宽度的乘积(面积) --> 削减特征量
+ 多项式回归(Polynomial regression)
* Computing Parameters Analytically
** Normal Equation 正规方程法
X = m * (n + 1) 1 --> x_0
\thata = (x^TX)^{-1}x^『T}y
X /设计矩阵 design matrix/
+ 不需要进行特征归一化
| Gradient Descent                | Normal Equation                      |
|---------------------------------+--------------------------------------|
| Need to chose \alpha            | No need to choose alpha              |
| Needs Many iterations           | Don't need to iterate                |
|---------------------------------+--------------------------------------|
| Works well even when n is large | Need to compute (X^{T}X)^{-1} -->    |
|                                 | SLOW if n i very large (x <=10000)   |
+ 对复杂的机器学习算法是没办法用正规方程法的

** Normal Equation Noninvertibility 正规方程的不可逆性
+ 当(X^{T}X)不可逆的时候
Octave  求逆
  + pinv -- pseudo-inverse 彭若斯广义逆
  + inv
+ Redundant feature 冗余的特征量
比如说两个特征联之间存在线性关系 则矩阵会不可逆
+ Too many features 特征量过多 甚至大于样本
+ Regularization



